{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random starting synaptic weights: \n",
      "[[-0.16595599]\n",
      " [ 0.44064899]\n",
      " [-0.99977125]]\n",
      "\n",
      "New synaptic weights after training: \n",
      "[[ 9.67299303]\n",
      " [-0.2078435 ]\n",
      " [-4.62963669]]\n",
      "\n",
      "Considering new situation [1, 0, 0] -> ?: \n",
      "[0.99993704]\n"
     ]
    }
   ],
   "source": [
    "from numpy import exp, array, random, dot\n",
    "\n",
    "class NeuralNetwork():\n",
    "    def __init__(self):\n",
    "        # Seed the random number generator, so it generates the same numbers\n",
    "        # every time the program runs.\n",
    "        random.seed(1)\n",
    "\n",
    "        # We model a single neuron, with 3 input connections and 1 output connection.\n",
    "        # We assign random weights to a 3 x 1 matrix, with values in the range -1 to 1\n",
    "        # and mean 0.\n",
    "        self.synaptic_weights = 2 * random.random((3, 1)) - 1\n",
    "\n",
    "    # The Sigmoid function, which describes an S shaped curve.\n",
    "    # We pass the weighted sum of the inputs through this function to\n",
    "    # normalise them between 0 and 1.\n",
    "    def __sigmoid(self, x):\n",
    "        return 1 / (1 + exp(-x))\n",
    "\n",
    "    # The derivative of the Sigmoid function.\n",
    "    # This is the gradient of the Sigmoid curve.\n",
    "    # It indicates how confident we are about the existing weight.\n",
    "    def __sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "\n",
    "    # We train the neural network through a process of trial and error.\n",
    "    # Adjusting the synaptic weights each time.\n",
    "    def train(self, training_set_inputs, training_set_outputs, number_of_training_iterations):\n",
    "        for iteration in range(number_of_training_iterations):\n",
    "            # Pass the training set through our neural network (a single neuron).\n",
    "            output = self.think(training_set_inputs)\n",
    "\n",
    "            # Calculate the error (The difference between the desired output\n",
    "            # and the predicted output).\n",
    "            error = training_set_outputs - output\n",
    "\n",
    "            # Multiply the error by the input and again by the gradient of the Sigmoid curve.\n",
    "            # This means less confident weights are adjusted more.\n",
    "            # This means inputs, which are zero, do not cause changes to the weights.\n",
    "            adjustment = dot(training_set_inputs.T, error * self.__sigmoid_derivative(output))\n",
    "\n",
    "            # Adjust the weights.\n",
    "            self.synaptic_weights += adjustment\n",
    "\n",
    "    # The neural network thinks.\n",
    "    def think(self, inputs):\n",
    "        # Pass inputs through our neural network (our single neuron).\n",
    "        return self.__sigmoid(dot(inputs, self.synaptic_weights))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    neural_network= NeuralNetwork()\n",
    "    \n",
    "    print(\"Random starting synaptic weights: \")\n",
    "    print(neural_network.synaptic_weights)\n",
    "\n",
    "    # The training set. We have 4 examples, each consisting of 3 input values\n",
    "    # and 1 output value.\n",
    "    training_set_inputs = array([[0, 0, 1], [1, 1, 1], [1, 0, 1], [0, 1, 1]])\n",
    "    training_set_outputs = array([[0, 1, 1, 0]]).T\n",
    "\n",
    "    \n",
    "    # Train the neural network using a training set.\n",
    "    # Do it 10,000 times and make small adjustments each time.\n",
    "    neural_network.train(training_set_inputs, training_set_outputs, 10000)\n",
    "\n",
    "    print(\"\\nNew synaptic weights after training: \")\n",
    "    print(neural_network.synaptic_weights)\n",
    "\n",
    "    \n",
    "    \n",
    "    # Test the neural network with a new situation.\n",
    "    print(\"\\nConsidering new situation [1, 0, 0] -> ?: \")\n",
    "    print(neural_network.think(array([1, 0, 0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  6.   , 148.   ,  72.   , ...,   0.627,  50.   ,   1.   ],\n",
       "       [  1.   ,  85.   ,  66.   , ...,   0.351,  31.   ,   0.   ],\n",
       "       [  8.   , 183.   ,  64.   , ...,   0.672,  32.   ,   1.   ],\n",
       "       ...,\n",
       "       [  5.   , 121.   ,  72.   , ...,   0.245,  30.   ,   0.   ],\n",
       "       [  1.   , 126.   ,  60.   , ...,   0.349,  47.   ,   1.   ],\n",
       "       [  1.   ,  93.   ,  70.   , ...,   0.315,  23.   ,   0.   ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(7)\n",
    "dataset=np.loadtxt(\"pima-indians-diabetes.csv\",delimiter=',')\n",
    "\n",
    "#spliting into input X and output Y\n",
    "X=dataset[:,0:8] # 8 input vars\n",
    "Y=dataset[:,8] # 1 output var\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0\n",
      "[1.5132220645030405, 0.7413611494732402, -0.5840508473019806]\n",
      "1 or 0 is : 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "a=1 #learning rate\n",
    "\n",
    "bias=1 #value of bias\n",
    "\n",
    "weights = [random.random(),random.random(),random.random()]\n",
    "# 3 weights consist 2 neuron and 1 bias\n",
    "  \n",
    "def perceptron(input1,input2,output):\n",
    "    outputP= input1*weights[0] + input2*weights[1]+bias*weights[2]\n",
    "    if outputP > 0:\n",
    "        outputP = 1\n",
    "    else:\n",
    "        outputP = 0\n",
    "    error= output - outputP\n",
    "    weights[0] += error * input1 * a\n",
    "    weights[1] += error * input2 * a\n",
    "    weights[2] += error * bias * a\n",
    "\n",
    "for i in range(50): \n",
    "    perceptron(1,1,1)#T or T\n",
    "    perceptron(1,0,1)#T or F\n",
    "    perceptron(0,1,1)#F or T\n",
    "    perceptron(0,0,0)#F or F\n",
    "    \n",
    "# training the model for given 3rd argument of perceptron    \n",
    "x=int(input())\n",
    "y= int(input())\n",
    "outputP = x*weights[0] + y*weights[1] + bias*weights[2]\n",
    "print(weights)\n",
    "if outputP > 0:\n",
    "    outputP=1\n",
    "else:\n",
    "    outputP=0\n",
    "print(x,\"or\",y,\"is :\",outputP)\n",
    "\n",
    "outputP = 1/(1+np.exp(-outputP)) #activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pima-indians dataset sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  6.   , 148.   ,  72.   , ...,   0.627,  50.   ,   1.   ],\n",
       "       [  1.   ,  85.   ,  66.   , ...,   0.351,  31.   ,   0.   ],\n",
       "       [  8.   , 183.   ,  64.   , ...,   0.672,  32.   ,   1.   ],\n",
       "       ...,\n",
       "       [  5.   , 121.   ,  72.   , ...,   0.245,  30.   ,   0.   ],\n",
       "       [  1.   , 126.   ,  60.   , ...,   0.349,  47.   ,   1.   ],\n",
       "       [  1.   ,  93.   ,  70.   , ...,   0.315,  23.   ,   0.   ]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import Sequential\n",
    "import numpy as np\n",
    "np.random.seed(7)\n",
    "dataset= np.loadtxt(\"pima-indians-diabetes.csv\",delimiter=\",\")\n",
    "#spliting into 8 input and 1 output\n",
    "X=dataset[:,0:8]\n",
    "Y=dataset[:,8]\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create model\n",
    "model=Sequential()\n",
    "model.add(Dense(12,input_dim=8,activation='relu'))\n",
    "model.add(Dense(8,activation='relu'))\n",
    "model.add(Dense(1,activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.sequential.Sequential at 0x266f4ad5dd8>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam',loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "768/768 [==============================] - 0s 401us/sample - loss: 0.5455 - acc: 0.7266\n",
      "Epoch 2/150\n",
      "768/768 [==============================] - 0s 343us/sample - loss: 0.5011 - acc: 0.7643\n",
      "Epoch 3/150\n",
      "768/768 [==============================] - 0s 377us/sample - loss: 0.5127 - acc: 0.7552\n",
      "Epoch 4/150\n",
      "768/768 [==============================] - 0s 414us/sample - loss: 0.5143 - acc: 0.7461\n",
      "Epoch 5/150\n",
      "768/768 [==============================] - 0s 473us/sample - loss: 0.4899 - acc: 0.7708\n",
      "Epoch 6/150\n",
      "768/768 [==============================] - 0s 303us/sample - loss: 0.4935 - acc: 0.7656\n",
      "Epoch 7/150\n",
      "768/768 [==============================] - 0s 491us/sample - loss: 0.5029 - acc: 0.7669\n",
      "Epoch 8/150\n",
      "768/768 [==============================] - 0s 419us/sample - loss: 0.4921 - acc: 0.7669\n",
      "Epoch 9/150\n",
      "768/768 [==============================] - 0s 253us/sample - loss: 0.4899 - acc: 0.7656\n",
      "Epoch 10/150\n",
      "768/768 [==============================] - 0s 291us/sample - loss: 0.4835 - acc: 0.7591\n",
      "Epoch 11/150\n",
      "768/768 [==============================] - 0s 334us/sample - loss: 0.4965 - acc: 0.7591\n",
      "Epoch 12/150\n",
      "768/768 [==============================] - 0s 537us/sample - loss: 0.4998 - acc: 0.7617\n",
      "Epoch 13/150\n",
      "768/768 [==============================] - 0s 334us/sample - loss: 0.4827 - acc: 0.7591\n",
      "Epoch 14/150\n",
      "768/768 [==============================] - 0s 274us/sample - loss: 0.5064 - acc: 0.7552\n",
      "Epoch 15/150\n",
      "768/768 [==============================] - 0s 276us/sample - loss: 0.5115 - acc: 0.7630\n",
      "Epoch 16/150\n",
      "768/768 [==============================] - 0s 332us/sample - loss: 0.5020 - acc: 0.7435\n",
      "Epoch 17/150\n",
      "768/768 [==============================] - 0s 398us/sample - loss: 0.4820 - acc: 0.7682\n",
      "Epoch 18/150\n",
      "768/768 [==============================] - 0s 346us/sample - loss: 0.4833 - acc: 0.7630\n",
      "Epoch 19/150\n",
      "768/768 [==============================] - 0s 458us/sample - loss: 0.4976 - acc: 0.7617\n",
      "Epoch 20/150\n",
      "768/768 [==============================] - 0s 328us/sample - loss: 0.4906 - acc: 0.7539\n",
      "Epoch 21/150\n",
      "768/768 [==============================] - 0s 328us/sample - loss: 0.4842 - acc: 0.7747\n",
      "Epoch 22/150\n",
      "768/768 [==============================] - 0s 389us/sample - loss: 0.4997 - acc: 0.7526\n",
      "Epoch 23/150\n",
      "768/768 [==============================] - 0s 572us/sample - loss: 0.5078 - acc: 0.7604\n",
      "Epoch 24/150\n",
      "768/768 [==============================] - 1s 958us/sample - loss: 0.5155 - acc: 0.7552\n",
      "Epoch 25/150\n",
      "768/768 [==============================] - 1s 984us/sample - loss: 0.4837 - acc: 0.7878\n",
      "Epoch 26/150\n",
      "768/768 [==============================] - 1s 1ms/sample - loss: 0.4883 - acc: 0.7565\n",
      "Epoch 27/150\n",
      "768/768 [==============================] - 1s 1ms/sample - loss: 0.4765 - acc: 0.7747 0s - loss: 0.4711 - acc: 0.\n",
      "Epoch 28/150\n",
      "768/768 [==============================] - 8s 10ms/sample - loss: 0.4832 - acc: 0.76825s - loss: 0\n",
      "Epoch 29/150\n",
      "768/768 [==============================] - 3s 4ms/sample - loss: 0.4857 - acc: 0.7656\n",
      "Epoch 30/150\n",
      "768/768 [==============================] - 1s 1ms/sample - loss: 0.4837 - acc: 0.7747\n",
      "Epoch 31/150\n",
      "768/768 [==============================] - 1s 1ms/sample - loss: 0.4892 - acc: 0.7682\n",
      "Epoch 32/150\n",
      "768/768 [==============================] - 1s 1ms/sample - loss: 0.5130 - acc: 0.7539\n",
      "Epoch 33/150\n",
      "768/768 [==============================] - 1s 824us/sample - loss: 0.4912 - acc: 0.7643\n",
      "Epoch 34/150\n",
      "768/768 [==============================] - 0s 503us/sample - loss: 0.4784 - acc: 0.7839\n",
      "Epoch 35/150\n",
      "768/768 [==============================] - 0s 453us/sample - loss: 0.4862 - acc: 0.7669\n",
      "Epoch 36/150\n",
      "768/768 [==============================] - 0s 401us/sample - loss: 0.4789 - acc: 0.7695\n",
      "Epoch 37/150\n",
      "768/768 [==============================] - 0s 358us/sample - loss: 0.4915 - acc: 0.7604\n",
      "Epoch 38/150\n",
      "768/768 [==============================] - 0s 349us/sample - loss: 0.4820 - acc: 0.7695\n",
      "Epoch 39/150\n",
      "768/768 [==============================] - 0s 410us/sample - loss: 0.4871 - acc: 0.7721\n",
      "Epoch 40/150\n",
      "768/768 [==============================] - 0s 551us/sample - loss: 0.4829 - acc: 0.7695\n",
      "Epoch 41/150\n",
      "768/768 [==============================] - 0s 378us/sample - loss: 0.4966 - acc: 0.7513\n",
      "Epoch 42/150\n",
      "768/768 [==============================] - 0s 318us/sample - loss: 0.4905 - acc: 0.7747\n",
      "Epoch 43/150\n",
      "768/768 [==============================] - 1s 737us/sample - loss: 0.5095 - acc: 0.7552\n",
      "Epoch 44/150\n",
      "768/768 [==============================] - 0s 359us/sample - loss: 0.4840 - acc: 0.7565\n",
      "Epoch 45/150\n",
      "768/768 [==============================] - 0s 428us/sample - loss: 0.4808 - acc: 0.7721\n",
      "Epoch 46/150\n",
      "768/768 [==============================] - 1s 728us/sample - loss: 0.4757 - acc: 0.7734\n",
      "Epoch 47/150\n",
      "768/768 [==============================] - 0s 409us/sample - loss: 0.5109 - acc: 0.7591\n",
      "Epoch 48/150\n",
      "768/768 [==============================] - 0s 582us/sample - loss: 0.4825 - acc: 0.7852\n",
      "Epoch 49/150\n",
      "768/768 [==============================] - 0s 388us/sample - loss: 0.4880 - acc: 0.7552\n",
      "Epoch 50/150\n",
      "768/768 [==============================] - 0s 298us/sample - loss: 0.4816 - acc: 0.7669\n",
      "Epoch 51/150\n",
      "768/768 [==============================] - 0s 281us/sample - loss: 0.4950 - acc: 0.7578\n",
      "Epoch 52/150\n",
      "768/768 [==============================] - 0s 370us/sample - loss: 0.4698 - acc: 0.7682\n",
      "Epoch 53/150\n",
      "768/768 [==============================] - 0s 320us/sample - loss: 0.4819 - acc: 0.7773\n",
      "Epoch 54/150\n",
      "768/768 [==============================] - 0s 341us/sample - loss: 0.4891 - acc: 0.7747\n",
      "Epoch 55/150\n",
      "768/768 [==============================] - 0s 298us/sample - loss: 0.4887 - acc: 0.7786\n",
      "Epoch 56/150\n",
      "768/768 [==============================] - 0s 374us/sample - loss: 0.4698 - acc: 0.7682\n",
      "Epoch 57/150\n",
      "768/768 [==============================] - 0s 539us/sample - loss: 0.4943 - acc: 0.7630\n",
      "Epoch 58/150\n",
      "768/768 [==============================] - 0s 556us/sample - loss: 0.4766 - acc: 0.7747\n",
      "Epoch 59/150\n",
      "768/768 [==============================] - 0s 388us/sample - loss: 0.4954 - acc: 0.7539\n",
      "Epoch 60/150\n",
      "768/768 [==============================] - ETA: 0s - loss: 0.4837 - acc: 0.757 - 0s 423us/sample - loss: 0.4870 - acc: 0.7539\n",
      "Epoch 61/150\n",
      "768/768 [==============================] - 0s 404us/sample - loss: 0.4920 - acc: 0.7578\n",
      "Epoch 62/150\n",
      "768/768 [==============================] - 0s 509us/sample - loss: 0.4856 - acc: 0.7852s - loss: 0.4296 - acc: 0\n",
      "Epoch 63/150\n",
      "768/768 [==============================] - 0s 447us/sample - loss: 0.4753 - acc: 0.7773\n",
      "Epoch 64/150\n",
      "768/768 [==============================] - 0s 546us/sample - loss: 0.4753 - acc: 0.7656\n",
      "Epoch 65/150\n",
      "768/768 [==============================] - 0s 421us/sample - loss: 0.4896 - acc: 0.7721\n",
      "Epoch 66/150\n",
      "768/768 [==============================] - 1s 760us/sample - loss: 0.4700 - acc: 0.7682\n",
      "Epoch 67/150\n",
      "768/768 [==============================] - 0s 624us/sample - loss: 0.4843 - acc: 0.7721\n",
      "Epoch 68/150\n",
      "768/768 [==============================] - 1s 714us/sample - loss: 0.4812 - acc: 0.7760\n",
      "Epoch 69/150\n",
      "768/768 [==============================] - 1s 747us/sample - loss: 0.4946 - acc: 0.7539\n",
      "Epoch 70/150\n",
      "768/768 [==============================] - 1s 697us/sample - loss: 0.4845 - acc: 0.7760\n",
      "Epoch 71/150\n",
      "768/768 [==============================] - 0s 600us/sample - loss: 0.5073 - acc: 0.7708\n",
      "Epoch 72/150\n",
      "768/768 [==============================] - 0s 645us/sample - loss: 0.4921 - acc: 0.7643\n",
      "Epoch 73/150\n",
      "768/768 [==============================] - 1s 697us/sample - loss: 0.4877 - acc: 0.7630\n",
      "Epoch 74/150\n",
      "768/768 [==============================] - 1s 660us/sample - loss: 0.4858 - acc: 0.7604\n",
      "Epoch 75/150\n",
      "768/768 [==============================] - 1s 707us/sample - loss: 0.4819 - acc: 0.7799\n",
      "Epoch 76/150\n",
      "768/768 [==============================] - 1s 1ms/sample - loss: 0.4824 - acc: 0.7799\n",
      "Epoch 77/150\n",
      "768/768 [==============================] - 1s 943us/sample - loss: 0.4883 - acc: 0.7695\n",
      "Epoch 78/150\n",
      "768/768 [==============================] - 0s 639us/sample - loss: 0.4830 - acc: 0.7669\n",
      "Epoch 79/150\n",
      "768/768 [==============================] - 0s 602us/sample - loss: 0.4739 - acc: 0.7695\n",
      "Epoch 80/150\n",
      "768/768 [==============================] - 0s 638us/sample - loss: 0.4745 - acc: 0.7773\n",
      "Epoch 81/150\n",
      "768/768 [==============================] - 1s 708us/sample - loss: 0.4729 - acc: 0.7669s - loss: 0.4357 - acc: 0.7 - ETA: 0s - loss: 0.4579 - acc: 0.\n",
      "Epoch 82/150\n",
      "768/768 [==============================] - 0s 393us/sample - loss: 0.4628 - acc: 0.7852\n",
      "Epoch 83/150\n",
      "768/768 [==============================] - 0s 513us/sample - loss: 0.4678 - acc: 0.7839\n",
      "Epoch 84/150\n",
      "768/768 [==============================] - 0s 438us/sample - loss: 0.4855 - acc: 0.7552\n",
      "Epoch 85/150\n",
      "768/768 [==============================] - 0s 612us/sample - loss: 0.4647 - acc: 0.7760s - loss: 0.4204 - acc\n",
      "Epoch 86/150\n",
      "768/768 [==============================] - 0s 425us/sample - loss: 0.4839 - acc: 0.7617\n",
      "Epoch 87/150\n",
      "768/768 [==============================] - 0s 439us/sample - loss: 0.4782 - acc: 0.7747\n",
      "Epoch 88/150\n",
      "768/768 [==============================] - 0s 396us/sample - loss: 0.4800 - acc: 0.7878s - loss: 0.4780 - acc: 0\n",
      "Epoch 89/150\n",
      "768/768 [==============================] - 0s 524us/sample - loss: 0.4832 - acc: 0.7721\n",
      "Epoch 90/150\n",
      "768/768 [==============================] - 0s 482us/sample - loss: 0.4707 - acc: 0.7656\n",
      "Epoch 91/150\n",
      "768/768 [==============================] - 0s 434us/sample - loss: 0.4824 - acc: 0.7604\n",
      "Epoch 92/150\n",
      "768/768 [==============================] - 0s 536us/sample - loss: 0.4798 - acc: 0.7669\n",
      "Epoch 93/150\n",
      "768/768 [==============================] - 0s 542us/sample - loss: 0.4719 - acc: 0.7669\n",
      "Epoch 94/150\n",
      "768/768 [==============================] - 0s 494us/sample - loss: 0.4776 - acc: 0.7682\n",
      "Epoch 95/150\n",
      "768/768 [==============================] - 0s 297us/sample - loss: 0.5006 - acc: 0.7630\n",
      "Epoch 96/150\n",
      "768/768 [==============================] - 0s 491us/sample - loss: 0.4748 - acc: 0.7852\n",
      "Epoch 97/150\n",
      "768/768 [==============================] - 0s 590us/sample - loss: 0.4797 - acc: 0.7591\n",
      "Epoch 98/150\n",
      "768/768 [==============================] - 0s 544us/sample - loss: 0.4719 - acc: 0.7682\n",
      "Epoch 99/150\n",
      "768/768 [==============================] - 0s 419us/sample - loss: 0.4686 - acc: 0.7708\n",
      "Epoch 100/150\n",
      "768/768 [==============================] - 0s 435us/sample - loss: 0.4761 - acc: 0.7682\n",
      "Epoch 101/150\n",
      "768/768 [==============================] - 0s 482us/sample - loss: 0.4687 - acc: 0.7682\n",
      "Epoch 102/150\n",
      "768/768 [==============================] - 0s 483us/sample - loss: 0.4787 - acc: 0.7760\n",
      "Epoch 103/150\n",
      "768/768 [==============================] - 1s 745us/sample - loss: 0.4655 - acc: 0.7826\n",
      "Epoch 104/150\n",
      "768/768 [==============================] - 0s 599us/sample - loss: 0.4838 - acc: 0.7656s - loss: 0.4400 - acc: \n",
      "Epoch 105/150\n",
      "768/768 [==============================] - 0s 520us/sample - loss: 0.4628 - acc: 0.7747\n",
      "Epoch 106/150\n",
      "768/768 [==============================] - 0s 470us/sample - loss: 0.4793 - acc: 0.7669\n",
      "Epoch 107/150\n",
      "768/768 [==============================] - 0s 360us/sample - loss: 0.4797 - acc: 0.7604\n",
      "Epoch 108/150\n",
      "768/768 [==============================] - 0s 420us/sample - loss: 0.4737 - acc: 0.7734\n",
      "Epoch 109/150\n",
      "768/768 [==============================] - 0s 503us/sample - loss: 0.4705 - acc: 0.7826\n",
      "Epoch 110/150\n",
      "768/768 [==============================] - 0s 379us/sample - loss: 0.4723 - acc: 0.7943\n",
      "Epoch 111/150\n",
      "768/768 [==============================] - 0s 348us/sample - loss: 0.4705 - acc: 0.7786\n",
      "Epoch 112/150\n",
      "768/768 [==============================] - 0s 345us/sample - loss: 0.4805 - acc: 0.7630\n",
      "Epoch 113/150\n",
      "768/768 [==============================] - 0s 331us/sample - loss: 0.4740 - acc: 0.7604\n",
      "Epoch 114/150\n",
      "768/768 [==============================] - 0s 396us/sample - loss: 0.4671 - acc: 0.7760\n",
      "Epoch 115/150\n",
      "768/768 [==============================] - 0s 326us/sample - loss: 0.4703 - acc: 0.7891\n",
      "Epoch 116/150\n",
      "768/768 [==============================] - 0s 395us/sample - loss: 0.4638 - acc: 0.7747\n",
      "Epoch 117/150\n",
      "768/768 [==============================] - 0s 385us/sample - loss: 0.4584 - acc: 0.7904\n",
      "Epoch 118/150\n",
      "768/768 [==============================] - 0s 405us/sample - loss: 0.4856 - acc: 0.7878\n",
      "Epoch 119/150\n",
      "768/768 [==============================] - 0s 509us/sample - loss: 0.4678 - acc: 0.7734\n",
      "Epoch 120/150\n",
      "768/768 [==============================] - 0s 513us/sample - loss: 0.4793 - acc: 0.7773\n",
      "Epoch 121/150\n",
      "768/768 [==============================] - 0s 556us/sample - loss: 0.4653 - acc: 0.7734\n",
      "Epoch 122/150\n",
      "768/768 [==============================] - 1s 703us/sample - loss: 0.4684 - acc: 0.7682\n",
      "Epoch 123/150\n",
      "768/768 [==============================] - 0s 417us/sample - loss: 0.4815 - acc: 0.7747\n",
      "Epoch 124/150\n",
      "768/768 [==============================] - 0s 620us/sample - loss: 0.4622 - acc: 0.7799\n",
      "Epoch 125/150\n",
      "768/768 [==============================] - 0s 441us/sample - loss: 0.4774 - acc: 0.7734\n",
      "Epoch 126/150\n",
      "768/768 [==============================] - 0s 428us/sample - loss: 0.5005 - acc: 0.7565\n",
      "Epoch 127/150\n",
      "768/768 [==============================] - 0s 491us/sample - loss: 0.4611 - acc: 0.7878\n",
      "Epoch 128/150\n",
      "768/768 [==============================] - 0s 477us/sample - loss: 0.4648 - acc: 0.7747\n",
      "Epoch 129/150\n",
      "768/768 [==============================] - 0s 404us/sample - loss: 0.4664 - acc: 0.7682\n",
      "Epoch 130/150\n",
      "768/768 [==============================] - 0s 495us/sample - loss: 0.4812 - acc: 0.7604\n",
      "Epoch 131/150\n",
      "768/768 [==============================] - 1s 840us/sample - loss: 0.4735 - acc: 0.7630\n",
      "Epoch 132/150\n",
      "768/768 [==============================] - 0s 417us/sample - loss: 0.4733 - acc: 0.7695\n",
      "Epoch 133/150\n",
      "768/768 [==============================] - 0s 461us/sample - loss: 0.4664 - acc: 0.7734\n",
      "Epoch 134/150\n",
      "768/768 [==============================] - 0s 559us/sample - loss: 0.4809 - acc: 0.7747\n",
      "Epoch 135/150\n",
      "768/768 [==============================] - 0s 551us/sample - loss: 0.4652 - acc: 0.7734\n",
      "Epoch 136/150\n",
      "768/768 [==============================] - 0s 441us/sample - loss: 0.4657 - acc: 0.7773\n",
      "Epoch 137/150\n",
      "768/768 [==============================] - 0s 516us/sample - loss: 0.4943 - acc: 0.7643\n",
      "Epoch 138/150\n",
      "768/768 [==============================] - 0s 477us/sample - loss: 0.4811 - acc: 0.7643\n",
      "Epoch 139/150\n",
      "768/768 [==============================] - 0s 572us/sample - loss: 0.4713 - acc: 0.7747\n",
      "Epoch 140/150\n",
      "768/768 [==============================] - 0s 410us/sample - loss: 0.4592 - acc: 0.7839\n",
      "Epoch 141/150\n",
      "768/768 [==============================] - 0s 494us/sample - loss: 0.4707 - acc: 0.7760\n",
      "Epoch 142/150\n",
      "768/768 [==============================] - 0s 456us/sample - loss: 0.4785 - acc: 0.7708\n",
      "Epoch 143/150\n",
      "768/768 [==============================] - 0s 454us/sample - loss: 0.4885 - acc: 0.7630\n",
      "Epoch 144/150\n",
      "768/768 [==============================] - 0s 572us/sample - loss: 0.4591 - acc: 0.7773\n",
      "Epoch 145/150\n",
      "768/768 [==============================] - 0s 353us/sample - loss: 0.4539 - acc: 0.7904\n",
      "Epoch 146/150\n",
      "768/768 [==============================] - 0s 313us/sample - loss: 0.4602 - acc: 0.7839\n",
      "Epoch 147/150\n",
      "768/768 [==============================] - 1s 686us/sample - loss: 0.5109 - acc: 0.7370\n",
      "Epoch 148/150\n",
      "768/768 [==============================] - 1s 690us/sample - loss: 0.4616 - acc: 0.7708\n",
      "Epoch 149/150\n",
      "768/768 [==============================] - 0s 645us/sample - loss: 0.4711 - acc: 0.7773\n",
      "Epoch 150/150\n",
      "768/768 [==============================] - 0s 548us/sample - loss: 0.4734 - acc: 0.7604\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x266f52dcc88>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X,Y,epochs=150,batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768/768 [==============================] - 0s 108us/sample - loss: 0.4579 - acc: 0.7721\n",
      "\n",
      "acc: 77.21%\n"
     ]
    }
   ],
   "source": [
    "scores= model.evaluate(X,Y)\n",
    "print(\"\\n%s: %.2f%%\" %(model.metrics_names[1],scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.45788536220788956, 0.77213544]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores\n",
    "# model.metrics_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768/768 [==============================] - 0s 36us/sample - loss: 0.4513 - acc: 0.8008\n",
      "Accuracy: 80.078125\n"
     ]
    }
   ],
   "source": [
    "scr= model.evaluate(X,Y)\n",
    "print(\"Accuracy:\",scr[1]*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
